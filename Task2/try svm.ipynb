{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global y_train\n",
    "    global x_train\n",
    "    global x_test\n",
    "    print(\"Load the data.\")\n",
    "    df_x_train = pd.read_csv('X_train.csv',header = 0 ,index_col=0)\n",
    "    df_y_train = pd.read_csv('y_train.csv',header = 0 ,index_col=0)\n",
    "    df_x_test = pd.read_csv('X_test.csv',header = 0 ,index_col=0)\n",
    "    x_train = df_x_train.values\n",
    "     \n",
    "    y_train = df_y_train['y'].values\n",
    "    x_test = df_x_test.values\n",
    "\n",
    "    # standardization\n",
    "    print('Standardize the data.')\n",
    "    scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "    \n",
    "    x_train =scaler.transform(x_train)\n",
    "    \n",
    "    x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print('Check training data.')\n",
    "# print(df_x_train.head())\n",
    "# print(df_x_train.shape)\n",
    "# print('There are %d NAN values.'%np.sum(np.isnan(x_train)))\n",
    "# print(df_y_train.groupby('y').size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Split data into training and validation sets.\")\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2,stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#feature selection\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest,f_classif,mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def feature_selection(x_train, y_train, x_test, method=\"rf\", max_features=200):\n",
    "    if(method == 'rf'):\n",
    "        sel = SelectFromModel(RandomForestClassifier(n_estimators=100,class_weight='balanced'), threshold=-np.inf, max_features=max_features)\n",
    "        sel.fit(x_train, y_train)\n",
    "        indicator = sel.get_support()\n",
    "        index = np.where(indicator==False)[0]\n",
    "\n",
    "        print(\"select %d features!\" % (indicator.shape[0] - index.shape[0]))\n",
    "        x_train_fs = np.delete(x_train, index, axis=1)\n",
    "        x_test_fs = np.delete(x_test, index, axis=1)\n",
    "        return x_train_fs, x_test_fs\n",
    "    \n",
    "    elif(method == 'xgb'):\n",
    "        sel = SelectFromModel(XGBRegressor(n_estimators=100, learning_rate=0.1, gamma=0.01, subsample=0.8, colsample_bytree=1, min_child_weight=5, max_depth=3), threshold=-np.inf, max_features=max_features)\n",
    "        sel.fit(x_train, y_train)\n",
    "        indicator = sel.get_support()\n",
    "        index = np.where(indicator==False)[0]\n",
    "\n",
    "        print(\"select %d features!\" % (indicator.shape[0] - index.shape[0]))\n",
    "        x_train_fs = np.delete(x_train, index, axis=1)\n",
    "        x_test_fs = np.delete(x_test, index, axis=1)\n",
    "        return x_train_fs, x_test_fs\n",
    "\n",
    "    elif(method == 'freg'):\n",
    "        #x_train = SelectKBest(f_regression, k=200).fit_transform(x_train, y_train)\n",
    "        sel = SelectKBest(f_classif, k=max_features)\n",
    "        sel.fit(x_train, y_train)\n",
    "        indicator = sel.get_support()\n",
    "        index = np.where(indicator==False)[0]\n",
    "\n",
    "        print(\"select %d features!\" % (indicator.shape[0] - index.shape[0]))\n",
    "        x_train_fs = np.delete(x_train, index, axis=1)\n",
    "        x_test_fs = np.delete(x_test, index, axis=1)\n",
    "        return x_train_fs, x_test_fs\n",
    "    \n",
    "    elif(method == 'mir'):\n",
    "        sel = SelectKBest(mutual_info_classif, k=max_features)\n",
    "        sel.fit(x_train, y_train)\n",
    "        indicator = sel.get_support()\n",
    "        index = np.where(indicator==False)[0]\n",
    "\n",
    "        print(\"select %d features!\" % (indicator.shape[0] - index.shape[0]))\n",
    "        x_train_fs = np.delete(x_train, index, axis=1)\n",
    "        x_test_fs = np.delete(x_test, index, axis=1)\n",
    "        return x_train_fs, x_test_fs\n",
    "    \n",
    "    elif(method == 'logr'):\n",
    "        sel = SelectFromModel(LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000))\n",
    "        sel.fit(x_train, y_train)\n",
    "        indicator = sel.get_support()\n",
    "        index = np.where(indicator==False)[0]\n",
    "\n",
    "        print(\"select %d features!\" % (indicator.shape[0] - index.shape[0]))\n",
    "        x_train_fs = np.delete(x_train, index, axis=1)\n",
    "        x_test_fs = np.delete(x_test, index, axis=1)\n",
    "        return x_train_fs, x_test_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "def outlier_detection(x_train, y_train, method='isoforest'):\n",
    "    if(method == 'isoforest'):\n",
    "        rng = np.random.RandomState(42)\n",
    "        clf = IsolationForest(behaviour='new', max_samples=1000,\n",
    "                          random_state=rng, \n",
    "                              contamination='auto')\n",
    "        clf.fit(x_train)\n",
    "        indicator = clf.predict(x_train)\n",
    "        index = np.where(indicator == -1)[0]\n",
    "        print(\"detect %d outliers in training set!\" % (index.shape[0]))\n",
    "\n",
    "        x_train_clean = np.delete(x_train, index, axis=0)\n",
    "        y_train_clean = np.delete(y_train, index, axis=0)\n",
    "\n",
    "        return x_train_clean, y_train_clean\n",
    "    \n",
    "    elif(method == 'lof'):\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "        clf.fit(x_train)\n",
    "        indicator = clf.fit_predict(x_train)\n",
    "        index = np.where(indicator == -1)[0]\n",
    "        print(\"detect %d outliers in training set!\" % (index.shape[0]))\n",
    "\n",
    "        x_train_clean = np.delete(x_train, index, axis=0)\n",
    "        y_train_clean = np.delete(y_train, index, axis=0)\n",
    "        return x_train_clean,y_train_clean\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "# clf = SVC(C=3.0,class_weight=None, \n",
    "#     decision_function_shape='ovr',  gamma='auto',\n",
    "#     max_iter=-1, probability=False, shrinking=True,\n",
    "#     tol=0.001, verbose=False)\n",
    "# clf.fit(x_train, y_train)\n",
    "# y_val_p = clf.predict(x_val)\n",
    "# val_result = balanced_accuracy_score(y_val_p,y_val)\n",
    "# print(val_result)\n",
    "# y_pred = clf.predict(fs_x_test)\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 450 features!\n",
      "detect 84 outliers in training set!\n",
      "0.75\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 457 features!\n",
      "detect 76 outliers in training set!\n",
      "0.6666666666666666\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 465 features!\n",
      "detect 73 outliers in training set!\n",
      "0.7222222222222222\n",
      "[1 0 1 ... 1 0 1]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 442 features!\n",
      "detect 62 outliers in training set!\n",
      "0.7777777777777777\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 443 features!\n",
      "detect 82 outliers in training set!\n",
      "0.7666666666666666\n",
      "[1 0 1 ... 1 0 1]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 467 features!\n",
      "detect 63 outliers in training set!\n",
      "0.9333333333333332\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 448 features!\n",
      "detect 66 outliers in training set!\n",
      "0.7999999999999999\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 458 features!\n",
      "detect 75 outliers in training set!\n",
      "0.48888888888888893\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 443 features!\n",
      "detect 66 outliers in training set!\n",
      "0.6666666666666666\n",
      "[1 0 1 ... 1 0 1]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 449 features!\n",
      "detect 61 outliers in training set!\n",
      "0.8333333333333334\n",
      "[1 0 1 ... 1 0 1]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 426 features!\n",
      "detect 75 outliers in training set!\n",
      "0.7777777777777778\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 439 features!\n",
      "detect 59 outliers in training set!\n",
      "0.6444444444444445\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 446 features!\n",
      "detect 66 outliers in training set!\n",
      "0.5333333333333333\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 457 features!\n",
      "detect 72 outliers in training set!\n",
      "0.5555555555555555\n",
      "[1 2 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 454 features!\n",
      "detect 72 outliers in training set!\n",
      "0.8333333333333334\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 440 features!\n",
      "detect 76 outliers in training set!\n",
      "0.3\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 450 features!\n",
      "detect 72 outliers in training set!\n",
      "0.9166666666666666\n",
      "[1 0 1 ... 1 0 1]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 449 features!\n",
      "detect 62 outliers in training set!\n",
      "0.7222222222222222\n",
      "[1 0 1 ... 1 0 1]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 447 features!\n",
      "detect 87 outliers in training set!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/bin/anaconda3/envs/amlenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "[1 0 1 ... 1 0 2]\n",
      "Load the data.\n",
      "Standardize the data.\n",
      "After down-sampling:\n",
      " y\n",
      "0    600\n",
      "1    600\n",
      "2    600\n",
      "dtype: int64\n",
      "select 455 features!\n",
      "detect 59 outliers in training set!\n",
      "0.7777777777777777\n",
      "[1 0 1 ... 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# subsampling\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "n=20\n",
    "for i in range(n):\n",
    "    load_data()\n",
    "    np.random.seed(i)\n",
    "#     model_smote = SMOTE()\n",
    "#     x_over_train, y_over_train = model_smote.fit_sample(x_train,y_train)\n",
    "\n",
    "    model_RandomUnderSampler = RandomUnderSampler() \n",
    "    x_down_train, y_down_train =model_RandomUnderSampler.fit_sample(x_train,y_train)\n",
    "#     print('After over-sampling:\\n',pd.DataFrame(y_over_train,columns=['y']).groupby('y').size())\n",
    "    print('After down-sampling:\\n',pd.DataFrame(y_down_train,columns=['y']).groupby('y').size())\n",
    "\n",
    "    fs_x_down_train,fs_x_test = feature_selection(x_down_train,y_down_train,x_test,\"logr\",500)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(fs_x_down_train, y_down_train, test_size=0.005)\n",
    "    x_train,y_train = outlier_detection(x_train,y_train,method='isoforest')\n",
    "\n",
    "\n",
    "    clf = SVC(C=3.0,class_weight=None, \n",
    "    decision_function_shape='ovr',  gamma='auto',\n",
    "    max_iter=-1, probability=False, shrinking=True,\n",
    "    tol=0.001, verbose=False,random_state=i)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_val_p = clf.predict(x_val)\n",
    "    val_result = balanced_accuracy_score(y_val_p,y_val)\n",
    "    print(val_result)\n",
    "    y_pred = clf.predict(fs_x_test)\n",
    "    print(y_pred)\n",
    "    \n",
    "    f = open(\"submission_svc{0:.1f}.csv\".format(i), \"w\")\n",
    "    f.write(\"id,y\\n\")\n",
    "    for i,x in enumerate(y_pred):\n",
    "        f.write(\"{},{}\\n\".format(i,x))\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result = [list() for i in range(4100)]\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    f = \"submission_svc%d.0.csv\" %(i)\n",
    "    f = open(f)\n",
    "    lines = f.readlines()\n",
    "    for l in lines[1:]:\n",
    "        l = l.strip().split(',')\n",
    "        idx, val = int(l[0]), int(float(l[1]))\n",
    "        result[idx].append(val)\n",
    "    f.close()\n",
    "print(result[:20])\n",
    "\n",
    "\n",
    "def vote(x):\n",
    "    c = [0] * 3\n",
    "    for i in x:\n",
    "        c[i] += 1\n",
    "    c = [(0, c[0]), (1, c[1]), (2, c[2])]\n",
    "    c.sort(key = lambda x: x[1], reverse = True)\n",
    "    if c[0][1] > c[1][1]:\n",
    "        return c[0][0]\n",
    "    else:\n",
    "        if c[0][0] == 1 or c[1][0] == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return np.random.choice([c[0][0], c[1][0]])\n",
    "print(vote([1,2,0,0,0]))\n",
    "\n",
    "\n",
    "with open(\"voted.csv\", \"w\") as f:\n",
    "    f.write(\"id,y\\n\")\n",
    "    for i in range(4100):\n",
    "        f.write(\"{},{}\\n\".format(i, vote(result[i])))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
