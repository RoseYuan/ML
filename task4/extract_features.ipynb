{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_x_test_eeg1 = pd.read_csv(\"test_eeg1.csv\") \n",
    "d_x_test_eeg2 = pd.read_csv(\"test_eeg2.csv\") \n",
    "d_x_test_emg = pd.read_csv(\"test_emg.csv\") \n",
    "d_x_train_eeg1 = pd.read_csv(\"train_eeg1.csv\")\n",
    "d_x_train_eeg2 = pd.read_csv(\"train_eeg2.csv\") \n",
    "d_x_train_emg = pd.read_csv(\"train_emg.csv\") \n",
    "d_y_train = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "\n",
    "x_test_eeg1 = d_x_test_eeg1.values[:,1:]\n",
    "x_test_eeg2 = d_x_test_eeg2.values[:,1:]\n",
    "x_test_emg = d_x_test_emg.values[:,1:]\n",
    "x_train_eeg1 = d_x_train_eeg1.values[:,1:]\n",
    "x_train_eeg2 = d_x_train_eeg2.values[:,1:]\n",
    "x_train_emg = d_x_train_emg.values[:,1:]\n",
    "y_train = d_y_train['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check training data.')\n",
    "print(d_x_train_eeg1.head())\n",
    "print(d_x_train_eeg1.shape)\n",
    "print('There are %d NAN values.'%np.sum(np.isnan(x_train_eeg1)))\n",
    "print(d_x_train_eeg2.head())\n",
    "print(d_x_train_eeg2.shape)\n",
    "print('There are %d NAN values.'%np.sum(np.isnan(x_train_eeg2)))\n",
    "print(d_x_train_emg.head())\n",
    "print(d_x_train_emg.shape)\n",
    "print('There are %d NAN values.'%np.sum(np.isnan(x_train_emg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_size = d_y_train.groupby('y').size()\n",
    "y1_size = d_y_train.iloc[0:21599,].groupby('y').size()\n",
    "y2_size = d_y_train.iloc[21600:43199,].groupby('y').size()\n",
    "y3_size = d_y_train.iloc[43200:64799,].groupby('y').size()\n",
    "print(\"y size:\",y_size,\n",
    "      \"\\n y1 size: \",y1_size,\n",
    "     \"\\n y2 size: \",y2_size,\n",
    "     \"\\n y3 size: \",y3_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train_eeg1=x_train_eeg1[0:21600,]\n",
    "x2_train_eeg1=x_train_eeg1[21600:43200,]\n",
    "x3_train_eeg1=x_train_eeg1[43200:64800,]\n",
    "x1_test_eeg1=x_test_eeg1[0:21600,]\n",
    "x2_test_eeg1=x_test_eeg1[21600:43200,]\n",
    "\n",
    "x1_train_eeg2=x_train_eeg2[0:21600,]\n",
    "x2_train_eeg2=x_train_eeg2[21600:43200,]\n",
    "x3_train_eeg2=x_train_eeg2[43200:64800,]\n",
    "x1_test_eeg2=x_test_eeg2[0:21600,]\n",
    "x2_test_eeg2=x_test_eeg2[21600:43200,]\n",
    "\n",
    "x1_train_emg=x_train_emg[0:21600,]\n",
    "x2_train_emg=x_train_emg[21600:43200,]\n",
    "x3_train_emg=x_train_emg[43200:64800,]\n",
    "x1_test_emg=x_test_emg[0:21600,]\n",
    "x2_test_emg=x_test_emg[21600:43200,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1=np.zeros((3,21600,512))\n",
    "x_1[0]=x1_train_eeg1\n",
    "x_1[1]=x1_train_eeg2\n",
    "x_1[2]=x1_train_emg\n",
    "\n",
    "x_2=np.zeros((3,21600,512))\n",
    "x_2[0]=x2_train_eeg1\n",
    "x_2[1]=x2_train_eeg2\n",
    "x_2[2]=x2_train_emg\n",
    "\n",
    "x_3=np.zeros((3,21600,512))\n",
    "x_3[0]=x3_train_eeg1\n",
    "x_3[1]=x3_train_eeg2\n",
    "x_3[2]=x3_train_emg\n",
    "\n",
    "x_1_test=np.zeros((3,21600,512))\n",
    "x_1_test[0]=x1_test_eeg1\n",
    "x_1_test[1]=x1_test_eeg2\n",
    "x_1_test[2]=x1_test_emg\n",
    "\n",
    "x_2_test=np.zeros((3,21600,512))\n",
    "x_2_test[0]=x2_test_eeg1\n",
    "x_2_test[1]=x2_test_eeg2\n",
    "x_2_test[2]=x2_test_emg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x1_train_eeg1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import integrate\n",
    "from biosppy.signals import eeg\n",
    "from scipy.signal import find_peaks,peak_prominences,peak_widths,periodogram\n",
    "from scipy.stats import kurtosis,skew\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def StatFeature(arrays):\n",
    "    mean = np.mean(arrays)\n",
    "    std = np.std(arrays)\n",
    "    maxv = np.max(arrays)\n",
    "    minv = np.min(arrays)\n",
    "    return [mean,std,maxv, minv]\n",
    "\n",
    "\n",
    "def findLFHF(psd, w):\n",
    "    VLFpsd = VLFw = LFpsd = LFw = HFpsd = HFw = np.empty(0)\n",
    "    m = w.shape[0]\n",
    "\n",
    "    for i in range(0, m):\n",
    "        if w[i] <= 0.05:\n",
    "            VLFpsd = np.append(VLFpsd, psd[i])\n",
    "            VLFw = np.append(VLFw, w[i])\n",
    "        if w[i] > 0.05 and w[i] <= 0.15:\n",
    "            LFpsd = np.append(LFpsd, psd[i])\n",
    "            LFw = np.append(LFw, w[i])\n",
    "        if w[i] > 0.15 and w[i] <= 0.4:\n",
    "            HFpsd = np.append(HFpsd, psd[i])\n",
    "            HFw = np.append(HFw, w[i])\n",
    "\n",
    "    LF = integrate.trapz(LFw, LFpsd) / (integrate.trapz(w, psd) - integrate.trapz(VLFw, VLFpsd))\n",
    "    HF = integrate.trapz(HFw, HFpsd) / (integrate.trapz(w, psd) - integrate.trapz(VLFw, VLFpsd))\n",
    "    LFHFratio = LF / HF\n",
    "    inter = LF / (LF + HF)\n",
    "    if HFpsd.size:\n",
    "        [maxHFD, maxIndex] = max((v, i) for i, v in enumerate(HFpsd))\n",
    "        FreqmaxP = HFw[maxIndex]\n",
    "    else:\n",
    "        maxHFD = 0\n",
    "        FreqmaxP = 0\n",
    "    return (LF, HF, FreqmaxP, maxHFD, LFHFratio, inter)\n",
    "\n",
    "\n",
    "def EMGFeatures(raw_signal, fs=128):\n",
    "    # Statistical Features\n",
    "    [_,std,maxv,minv] = StatFeature(raw_signal)\n",
    "    \n",
    "    # Power Spectrum\n",
    "    w = np.hamming(len(raw_signal))       # using Hamming window function\n",
    "    w, psd = periodogram(raw_signal, window=w, detrend=False)\n",
    "    _, _, _, maxHFD, _, _ = findLFHF(psd, w)\n",
    "    \n",
    "    # Time Series\n",
    "    kurt = kurtosis(raw_signal)\n",
    "    sk = skew(raw_signal)\n",
    "        \n",
    "    # Peak Features\n",
    "    [peaks,_] = find_peaks(raw_signal)\n",
    "    pprom = peak_prominences(raw_signal,peaks)[0]\n",
    "    contour_heights = raw_signal[peaks] - pprom\n",
    "    pwid = peak_widths(raw_signal,peaks,rel_height=0.4)[0]\n",
    "    [ppmean,ppstd,_,ppmin] = StatFeature(pprom)\n",
    "    [pwmean,pwstd,pwmax,pwmin] = StatFeature(pwid)\n",
    " \n",
    "    return np.array([std,maxv,minv,maxHFD, kurt,sk,ppmean,ppstd,ppmin,pwmean,pwstd,pwmax,pwmin])\n",
    "\n",
    "\n",
    "def EEGFeatures(raw_signal,signal_1,signal_2, fs=128):\n",
    "    bio_signal = np.transpose(raw_signal)\n",
    "        \n",
    "    # Statistical Features\n",
    "    [_,std1,maxv1,minv1] = StatFeature(signal_1)\n",
    "    [_,std2,maxv2,minv2] = StatFeature(signal_2)    \n",
    "        \n",
    "    # Power Features\n",
    "    [_, theta, alpha_low,alpha_high,beta, gamma]= eeg.get_power_features(signal=bio_signal, sampling_rate=fs)\n",
    "    # seems not good because it use smaller windows to calculate the power\n",
    "    # don't know how this function is implemented in details\n",
    "        \n",
    "    [theta1_mean,theta1_std,theta1_max,theta1_min] = StatFeature(theta[:,0])\n",
    "    [theta2_mean,theta2_std,theta2_max,theta2_min] = StatFeature(theta[:,1])\n",
    "    [alpha_low1_mean,alpha_low1_std,alpha_low1_max,alpha_low1_min] = StatFeature(alpha_low[:,0])\n",
    "    [alpha_low2_mean,alpha_low2_std,alpha_low2_max,alpha_low2_min] = StatFeature(alpha_low[:,1])\n",
    "    [alpha_high1_mean,alpha_high1_std,alpha_high1_max,alpha_high1_min] = StatFeature(alpha_high[:,0])\n",
    "    [alpha_high2_mean,alpha_high2_std,alpha_high2_max,alpha_high2_min] = StatFeature(alpha_high[:,1])\n",
    "    [beta1_mean,beta1_std,beta1_max,beta1_min] = StatFeature(beta[:,0])\n",
    "    [beta2_mean,beta2_std,beta2_max,beta2_min] = StatFeature(beta[:,1])\n",
    "    [gamma1_mean,gamma1_std,gamma1_max,gamma1_min] = StatFeature(gamma[:,0])\n",
    "    [gamma2_mean,gamma2_std,gamma2_max,gamma2_min] = StatFeature(gamma[:,1])\n",
    "        \n",
    "        \n",
    "    # Power Spectrum\n",
    "    w = np.hamming(len(signal_1))\n",
    "    w, psd = periodogram(signal_1, window=w, detrend=False)\n",
    "    _, _, FreqmaxP1, _, _, _ = findLFHF(psd, w)\n",
    "    w = np.hamming(len(signal_2))\n",
    "    w, psd = periodogram(signal_2, window=w, detrend=False)\n",
    "    _, _, FreqmaxP2, _, _, _ = findLFHF(psd, w)\n",
    "        \n",
    "    # Time Series\n",
    "    kurt1 = kurtosis(signal_1)\n",
    "    skew1 = skew(signal_1)\n",
    "    kurt2 = kurtosis(signal_2)\n",
    "    skew2 = skew(signal_2)\n",
    "        \n",
    "    # Peak Features\n",
    "    [peaks1,_] = find_peaks(signal_1)\n",
    "    pprom1 = peak_prominences(signal_1,peaks1)[0]\n",
    "    contour_heights1 = signal_1[peaks1] - pprom1\n",
    "    pwid1 = peak_widths(signal_1,peaks1,rel_height=0.4)[0]\n",
    "    [ppmean1,ppstd1,_,ppmin1] = StatFeature(pprom1)\n",
    "    [pwmean1,pwstd1,pwmax1,pwmin1] = StatFeature(pwid1)\n",
    "        \n",
    "    [peaks2,_] = find_peaks(signal_2)\n",
    "    pprom2 = peak_prominences(signal_2,peaks2)[0]\n",
    "    contour_heights2 = signal_2[peaks2] - pprom2\n",
    "    pwid2 = peak_widths(signal_2,peaks2,rel_height=0.4)[0]\n",
    "    [ppmean2,ppstd2,_,ppmin2] = StatFeature(pprom2)\n",
    "    [pwmean2,pwstd2,pwmax2,pwmin2] = StatFeature(pwid2)\n",
    " \n",
    "    return np.array([theta1_mean,theta1_std,theta1_max,theta1_min, theta2_mean,theta2_std,theta2_max,theta2_min, alpha_low1_mean,alpha_low1_std,alpha_low1_max,alpha_low1_min,alpha_low2_mean,alpha_low2_std,alpha_low2_max,alpha_low2_min,alpha_high1_mean,alpha_high1_std,alpha_high1_max,alpha_high1_min,alpha_high2_mean,alpha_high2_std,alpha_high2_max,alpha_high2_min,beta1_mean,beta1_std,beta1_max,beta1_min,beta2_mean,beta2_std,beta2_max,beta2_min,gamma1_mean,gamma1_std,gamma1_max,gamma1_min,gamma2_mean,gamma2_std,gamma2_max,gamma2_min,FreqmaxP1,kurt1,skew1,ppmean1,ppstd1,ppmin1,pwmean1,pwstd1,pwmax1,pwmin1,FreqmaxP2,kurt2,skew2,ppmean2,ppstd2,ppmin2,pwmean2,pwstd2,pwmax2,pwmin2,std1,maxv1,minv1,std2,maxv2,minv2])\n",
    "\n",
    "\n",
    "def ExtractFeatures(eeg1,eeg2,emg, fs=128):\n",
    "    raw_signal = np.concatenate(([eeg1],[eeg2]))\n",
    "    feature1 = EEGFeatures(raw_signal,eeg1,eeg2,fs=fs)\n",
    "    feature2 = EMGFeatures(emg)\n",
    "    return np.concatenate((feature1, feature2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, psd = periodogram(np.concatenate(([x1_train_eeg1[0]],[x1_train_eeg2[0]])), detrend=False)\n",
    "np.concatenate((x1_train_eeg1[0],x1_train_eeg2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features according to https://www.sciencedirect.com/science/article/pii/S0010482512001588\n",
    "\n",
    "def AddExtractFeatures(eeg1,eeg2,emg,fs=128):\n",
    "    raw_signal = np.concatenate((eeg1,eeg2))\n",
    "    w = np.hamming(raw_signal.shape[0])       # using Hamming window function\n",
    "    w, psd = periodogram(raw_signal, window=w, detrend=False)\n",
    "    betapsd = betaw = alphapsd = alphaw = deltapsd = deltaw = thetapsd = thetaw = gammapsd = gammaw = np.empty(0)\n",
    "    m = w.shape[0]\n",
    "    for i in range(0, m):\n",
    "        if w[i]>0.5 and w[i] <= 4:\n",
    "            deltapsd = np.append(deltapsd, psd[i])\n",
    "            deltaw = np.append(deltaw, w[i])\n",
    "        if w[i] > 4 and w[i] <= 8:\n",
    "            thetapsd = np.append(thetapsd, psd[i])\n",
    "            thetaw = np.append(thetaw, w[i])\n",
    "        if w[i] > 8 and w[i] <= 12:\n",
    "            alphapsd = np.append(alphapsd, psd[i])\n",
    "            alphaw = np.append(alphaw, w[i])\n",
    "        if w[i] > 13 and w[i] <= 30:\n",
    "            betapsd = np.append(betapsd, psd[i])\n",
    "            betaw = np.append(betaw, w[i])\n",
    "        if w[i] > 30:\n",
    "            gammapsd = np.append(gammapsd, psd[i])\n",
    "            gammaw = np.append(gammaw, w[i])\n",
    "    # averaged power for each frequency bands [delta_p,theta_p,alpha_p,beta_p,gamma_p]\n",
    "    delta_p = np.mean(deltapsd)\n",
    "    theta_p = np.mean(thetapsd)\n",
    "    alpha_p = np.mean(alphapsd)\n",
    "    beta_p = np.mean(betapsd)\n",
    "    gamma_p = np.mean(gammapsd)\n",
    "    overall_p = np.mean(psd)\n",
    "    # relative power for each frequency bands\n",
    "    r_delta_p = delta_p/overall_p\n",
    "    r_theta_p = theta_p/overall_p\n",
    "    r_alpha_p = alpha_p/overall_p\n",
    "    r_beta_p = beta_p/overall_p\n",
    "    r_gamma_p = gamma_p/overall_p\n",
    "    # power ratios (δ/α), (δ/β),(δ/θ),(θ/α),(θ/β),(α/β),(δ+θ)/(α+β)\n",
    "    r1 = delta_p/alpha_p\n",
    "    r2 = delta_p/beta_p\n",
    "    r3 = delta_p/theta_p\n",
    "    r4 = theta_p/alpha_p\n",
    "    r5 = theta_p/beta_p\n",
    "    r6 = alpha_p/beta_p\n",
    "    r7 = (delta_p+theta_p)/(alpha_p+beta_p)\n",
    "    # SEF95\n",
    "    cumulated_p = 0\n",
    "    th = sum(psd)*0.95\n",
    "    for i,f in enumerate(w):\n",
    "        cumulated_p = cumulated_p + psd[i]\n",
    "        if cumulated_p > th:\n",
    "            sef95 = f\n",
    "            break\n",
    "# time domain\n",
    "    # 75th percentile\n",
    "    per_75_1 = np.quantile(eeg1, .75)\n",
    "    per_75_2 = np.quantile(eeg2, .75)\n",
    "    # zero crossing rate\n",
    "    window_s = 0\n",
    "    window_e = 201\n",
    "    ZCRs1 = []\n",
    "    ZCRs2 = []\n",
    "    for i in range(213):\n",
    "        zero1 = np.mean(eeg1[window_s:window_e])\n",
    "        zcr1 = 0\n",
    "        zero2 = np.mean(eeg2[window_s:window_e])\n",
    "        zcr2 = 0\n",
    "        for j in range(window_s,window_e):\n",
    "            if (eeg1[j]-zero1)*(eeg1[j+1]-zero1)<0:\n",
    "                zcr1 += 1\n",
    "            if (eeg2[j]-zero2)*(eeg2[j+1]-zero2)<0:\n",
    "                zcr2 += 1\n",
    "        ZCRs1.append(zcr1)\n",
    "        ZCRs2.append(zcr2)\n",
    "    [zmean1,zstd1,zmaxv1,zminv1] = StatFeature(ZCRs1)\n",
    "    [zmean2,zstd2,zmaxv2,zminv2] = StatFeature(ZCRs2)\n",
    "    return np.array([delta_p,theta_p,alpha_p,beta_p,gamma_p,r_delta_p,overall_p,r_theta_p,r_alpha_p,r_beta_p,r_gamma_p,r1,r2,r3,r4,r5,r6,r7,sef95,per_75_1,per_75_2,zmean1,zstd1,zmaxv1,zminv1,zmean2,zstd2,zmaxv2,zminv2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5,6,7])\n",
    "a[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile([1,2,3,4,4,4],0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_signal=np.concatenate(([x_1[0][0]],[x_1[1][0]])).transpose() \n",
    "bio_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_, theta, alpha_low,alpha_high,beta, gamma]=eeg.get_power_features(signal=bio_signal, sampling_rate=128,size=0.5)\n",
    "gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_1_features=[]\n",
    "x_2_features=[]\n",
    "x_3_features=[]\n",
    "x_2_test_features=[]\n",
    "x_1_test_features=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [x_1,x_2,x_3,x_1_test,x_2_test]\n",
    "subjects_f = [x_1_features,x_2_features,x_3_features,x_1_test_features,x_2_test_features]\n",
    "\n",
    "for j,s in enumerate(subjects):\n",
    "    for i in range(21600):\n",
    "        if(i%200==0):\n",
    "                print(\"Processing the \",i/200,\" batch of 200 signals...\")\n",
    "        raw_signal = np.concatenate(([s[0][i]],[s[1][i]]))\n",
    "        feature0 = ExtractFeatures(np.array(s[0][i]),np.array(s[1][i]),np.array(s[2][i]))\n",
    "        subjects_f[j].append(feature0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(21600):\n",
    "    if(i%200==0):\n",
    "            print(\"Processing the \",i/200,\" batch of 200 signals...\")\n",
    "    raw_signal = np.concatenate(([x_2[0][i]],[x_2[1][i]]))\n",
    "    feature0 = ExtractFeatures(np.array(x_2[0][i]),np.array(x_2[1][i]),np.array(x_2[2][i]))\n",
    "    x_2_features.append(feature0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(21600):\n",
    "    if(i%200==0):\n",
    "            print(\"Processing the \",i/200,\" batch of 200 signals...\")\n",
    "#     raw_signal = np.concatenate(([x_3[0][i]],[x_3[1][i]]))\n",
    "    feature0 = ExtractFeatures(np.array(x_3[0][i]),np.array(x_3[1][i]),np.array(x_3[2][i]))\n",
    "    x_3_features.append(feature0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(21600):\n",
    "    if(i%200==0):\n",
    "            print(\"Processing the \",i/200,\" batch of 200 signals...\")\n",
    "#     raw_signal = np.concatenate(([x_1_test[0][i]],[x_1_test[1][i]]))\n",
    "    feature0 = ExtractFeatures(np.array(x_1_test[0][i]),np.array(x_1_test[1][i]),np.array(x_1_test[2][i]))\n",
    "    x_1_test_features.append(feature0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(21600):\n",
    "    if(i%200==0):\n",
    "            print(\"Processing the \",i/200,\" batch of 200 signals...\")\n",
    "#     raw_signal = np.concatenate(([x_2_test[0][i]],[x_2_test[1][i]]))\n",
    "    feature0 = ExtractFeatures(np.array(x_2_test[0][i]),np.array(x_2_test[1][i]),np.array(x_2_test[2][i]))\n",
    "    x_2_test_features.append(feature0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file_features(feature_M):\n",
    "    pd.DataFrame(eval(feature_M)).to_csv(\"%s.csv\"%(feature_M),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file_features(\"x_1_features\")\n",
    "write_to_file_features(\"x_2_features\")\n",
    "write_to_file_features(\"x_3_features\")\n",
    "write_to_file_features(\"x_1_test_features\")\n",
    "write_to_file_features(\"x_2_test_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1_features = np.array(x_1_features)\n",
    "x_2_features = np.array(x_2_features)\n",
    "x_3_features = np.array(x_3_features)\n",
    "x_1_test_features = np.array(x_1_test_features)\n",
    "x_2_test_features = np.array(x_2_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_1_features = x_1_features[:,0:79]\n",
    "# x_2_features = x_2_features[:,0:79]\n",
    "# x_3_features = x_3_features[:,0:79]\n",
    "# x_1_test_features = x_1_test_features[:,0:79]\n",
    "# x_2_test_features = x_2_test_features[:,0:79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subjects = [x_1,x_2,x_3,x_1_test,x_2_test]\n",
    "subjects_f = [x_1_features,x_2_features,x_3_features,x_1_test_features,x_2_test_features]\n",
    "add_to_all_s = []\n",
    "for s in subjects:\n",
    "    listf = []\n",
    "    for i in range(21600):\n",
    "        if(i%200==0):\n",
    "                print(\"Processing the \",i/200,\" batch of 200 signals...\")\n",
    "        raw_signal = np.concatenate(([s[0][i]],[s[1][i]]))\n",
    "        feature_add = AddExtractFeatures(np.array(s[0][i]),np.array(s[1][i]),np.array(s[2][i]))\n",
    "        listf.append(feature_add)\n",
    "    add_to_all_s.append(listf)\n",
    "x_1_features_a = np.concatenate((x_1_features,add_to_all_s[0]),axis=1)\n",
    "x_2_features_a = np.concatenate((x_2_features,add_to_all_s[1]),axis=1)\n",
    "x_3_features_a = np.concatenate((x_3_features,add_to_all_s[2]),axis=1)\n",
    "x_1_test_features_a = np.concatenate((x_1_test_features,add_to_all_s[3]),axis=1)\n",
    "x_2_test_features_a = np.concatenate((x_2_test_features,add_to_all_s[4]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the  2.0  batch of 200 signals...\n",
      "Processing the  3.0  batch of 200 signals...\n",
      "Processing the  4.0  batch of 200 signals...\n",
      "Processing the  5.0  batch of 200 signals...\n",
      "Processing the  6.0  batch of 200 signals...\n",
      "Processing the  7.0  batch of 200 signals...\n",
      "Processing the  8.0  batch of 200 signals...\n",
      "Processing the  9.0  batch of 200 signals...\n",
      "Processing the  10.0  batch of 200 signals...\n",
      "Processing the  11.0  batch of 200 signals...\n",
      "Processing the  12.0  batch of 200 signals...\n",
      "Processing the  13.0  batch of 200 signals...\n",
      "Processing the  14.0  batch of 200 signals...\n",
      "Processing the  15.0  batch of 200 signals...\n",
      "Processing the  16.0  batch of 200 signals...\n",
      "Processing the  17.0  batch of 200 signals...\n",
      "Processing the  18.0  batch of 200 signals...\n",
      "Processing the  19.0  batch of 200 signals...\n",
      "Processing the  20.0  batch of 200 signals...\n",
      "Processing the  21.0  batch of 200 signals...\n",
      "Processing the  22.0  batch of 200 signals...\n",
      "Processing the  23.0  batch of 200 signals...\n",
      "Processing the  24.0  batch of 200 signals...\n",
      "Processing the  25.0  batch of 200 signals...\n",
      "Processing the  26.0  batch of 200 signals...\n",
      "Processing the  27.0  batch of 200 signals...\n",
      "Processing the  28.0  batch of 200 signals...\n",
      "Processing the  29.0  batch of 200 signals...\n",
      "Processing the  30.0  batch of 200 signals...\n",
      "Processing the  31.0  batch of 200 signals...\n",
      "Processing the  32.0  batch of 200 signals...\n",
      "Processing the  33.0  batch of 200 signals...\n",
      "Processing the  34.0  batch of 200 signals...\n",
      "Processing the  35.0  batch of 200 signals...\n",
      "Processing the  36.0  batch of 200 signals...\n",
      "Processing the  37.0  batch of 200 signals...\n",
      "Processing the  38.0  batch of 200 signals...\n",
      "Processing the  39.0  batch of 200 signals...\n",
      "Processing the  40.0  batch of 200 signals...\n",
      "Processing the  41.0  batch of 200 signals...\n",
      "Processing the  42.0  batch of 200 signals...\n",
      "Processing the  43.0  batch of 200 signals...\n",
      "Processing the  44.0  batch of 200 signals...\n",
      "Processing the  45.0  batch of 200 signals...\n",
      "Processing the  46.0  batch of 200 signals...\n",
      "Processing the  47.0  batch of 200 signals...\n",
      "Processing the  48.0  batch of 200 signals...\n",
      "Processing the  49.0  batch of 200 signals...\n",
      "Processing the  50.0  batch of 200 signals...\n",
      "Processing the  51.0  batch of 200 signals...\n",
      "Processing the  52.0  batch of 200 signals...\n",
      "Processing the  53.0  batch of 200 signals...\n",
      "Processing the  54.0  batch of 200 signals...\n",
      "Processing the  55.0  batch of 200 signals...\n",
      "Processing the  56.0  batch of 200 signals...\n",
      "Processing the  57.0  batch of 200 signals...\n",
      "Processing the  58.0  batch of 200 signals...\n",
      "Processing the  59.0  batch of 200 signals...\n",
      "Processing the  60.0  batch of 200 signals...\n",
      "Processing the  61.0  batch of 200 signals...\n",
      "Processing the  62.0  batch of 200 signals...\n",
      "Processing the  63.0  batch of 200 signals...\n",
      "Processing the  64.0  batch of 200 signals...\n",
      "Processing the  65.0  batch of 200 signals...\n",
      "Processing the  66.0  batch of 200 signals...\n",
      "Processing the  67.0  batch of 200 signals...\n",
      "Processing the  68.0  batch of 200 signals...\n",
      "Processing the  69.0  batch of 200 signals...\n",
      "Processing the  70.0  batch of 200 signals...\n",
      "Processing the  71.0  batch of 200 signals...\n",
      "Processing the  72.0  batch of 200 signals...\n",
      "Processing the  73.0  batch of 200 signals...\n",
      "Processing the  74.0  batch of 200 signals...\n",
      "Processing the  75.0  batch of 200 signals...\n",
      "Processing the  76.0  batch of 200 signals...\n",
      "Processing the  77.0  batch of 200 signals...\n",
      "Processing the  78.0  batch of 200 signals...\n",
      "Processing the  79.0  batch of 200 signals...\n",
      "Processing the  80.0  batch of 200 signals...\n",
      "Processing the  81.0  batch of 200 signals...\n",
      "Processing the  82.0  batch of 200 signals...\n",
      "Processing the  83.0  batch of 200 signals...\n",
      "Processing the  84.0  batch of 200 signals...\n",
      "Processing the  85.0  batch of 200 signals...\n",
      "Processing the  86.0  batch of 200 signals...\n",
      "Processing the  87.0  batch of 200 signals...\n",
      "Processing the  88.0  batch of 200 signals...\n",
      "Processing the  89.0  batch of 200 signals...\n",
      "Processing the  90.0  batch of 200 signals...\n",
      "Processing the  91.0  batch of 200 signals...\n",
      "Processing the  92.0  batch of 200 signals...\n",
      "Processing the  93.0  batch of 200 signals...\n",
      "Processing the  94.0  batch of 200 signals...\n",
      "Processing the  95.0  batch of 200 signals...\n",
      "Processing the  96.0  batch of 200 signals...\n",
      "Processing the  97.0  batch of 200 signals...\n",
      "Processing the  98.0  batch of 200 signals...\n",
      "Processing the  99.0  batch of 200 signals...\n",
      "Processing the  100.0  batch of 200 signals...\n",
      "Processing the  101.0  batch of 200 signals...\n",
      "Processing the  102.0  batch of 200 signals...\n",
      "Processing the  103.0  batch of 200 signals...\n",
      "Processing the  104.0  batch of 200 signals...\n",
      "Processing the  105.0  batch of 200 signals...\n",
      "Processing the  106.0  batch of 200 signals...\n",
      "Processing the  107.0  batch of 200 signals...\n",
      "Processing the  0.0  batch of 200 signals...\n",
      "Processing the  1.0  batch of 200 signals...\n",
      "Processing the  2.0  batch of 200 signals...\n",
      "Processing the  3.0  batch of 200 signals...\n",
      "Processing the  4.0  batch of 200 signals...\n",
      "Processing the  5.0  batch of 200 signals...\n",
      "Processing the  6.0  batch of 200 signals...\n",
      "Processing the  7.0  batch of 200 signals...\n",
      "Processing the  8.0  batch of 200 signals...\n",
      "Processing the  9.0  batch of 200 signals...\n",
      "Processing the  10.0  batch of 200 signals...\n",
      "Processing the  11.0  batch of 200 signals...\n",
      "Processing the  12.0  batch of 200 signals...\n",
      "Processing the  13.0  batch of 200 signals...\n",
      "Processing the  14.0  batch of 200 signals...\n",
      "Processing the  15.0  batch of 200 signals...\n",
      "Processing the  16.0  batch of 200 signals...\n",
      "Processing the  17.0  batch of 200 signals...\n",
      "Processing the  18.0  batch of 200 signals...\n",
      "Processing the  19.0  batch of 200 signals...\n",
      "Processing the  20.0  batch of 200 signals...\n",
      "Processing the  21.0  batch of 200 signals...\n",
      "Processing the  22.0  batch of 200 signals...\n",
      "Processing the  23.0  batch of 200 signals...\n",
      "Processing the  24.0  batch of 200 signals...\n",
      "Processing the  25.0  batch of 200 signals...\n",
      "Processing the  26.0  batch of 200 signals...\n",
      "Processing the  27.0  batch of 200 signals...\n",
      "Processing the  28.0  batch of 200 signals...\n"
     ]
    }
   ],
   "source": [
    "subjects = [x_1,x_2,x_3,x_1_test,x_2_test]\n",
    "\n",
    "add_to_all_s = []\n",
    "for s in subjects:\n",
    "    listf = []\n",
    "    for i in range(21600):\n",
    "        if(i%200==0):\n",
    "                print(\"Processing the \",i/200,\" batch of 200 signals...\")\n",
    "        raw_signal = np.concatenate(([s[0][i]],[s[1][i]]))\n",
    "        feature_add = AddExtractFeatures(np.array(s[0][i]),np.array(s[1][i]),np.array(s[2][i]))\n",
    "        listf.append(feature_add)\n",
    "    add_to_all_s.append(listf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1_features_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file_features(\"add_to_all_s[0]\")\n",
    "write_to_file_features(\"add_to_all_s[1]\")\n",
    "write_to_file_features(\"add_to_all_s[2]\")\n",
    "write_to_file_features(\"add_to_all_s[3]\")\n",
    "write_to_file_features(\"add_to_all_s[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file_features(\"x_1_features_a\")\n",
    "write_to_file_features(\"x_2_features_a\")\n",
    "write_to_file_features(\"x_3_features_a\")\n",
    "write_to_file_features(\"x_1_test_features_a\")\n",
    "write_to_file_features(\"x_2_test_features_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
